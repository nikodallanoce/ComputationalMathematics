\section{Introduction}
In this section we will provide an inside view about the task by showing a brief description of it, its convexity study and Lipschitz continuity, needed to introduce the properties of the algorithms over the next sections.

\subsection{Linear Least Squares}\label{subsec:introduction_lls}
Linear least squares problem (we will call it \textbf{LLS} for the rest of the report) is an optimization problem in which we need to find a vector $x\in \mathbb{R}^n$ such that it minimizes $\lVert Ax-b\rVert_2$, formally
\begin{equation}
\displaystyle \min_{x}f(x) \hspace{3mm} \text{where} \hspace{3mm}f(x)=\lVert Ax-b\rVert_2
\end{equation}
with:
\begin{itemize}
    \item $A \in \mathbb{R}^{m\times n}$ is a tall thin matrix where, typically, $m>>n$, like in a model fitting setting.
    \item $b \in \mathbb{R}^m$ is a vector, or, using a model fitting vocabulary, the expected values.
    \item $\lVert x \rVert_2$, is the euclidean norm (or 2-norm) such that $\lVert x \rVert_2=\sqrt{\sum_{i}{x_{i}}^2}=\sqrt{x^Tx}$.
\end{itemize}
by keeping in mind that the square root function is monotone, minimizing this function is equal to minimize its argument, formally $\displaystyle \min_x \sqrt{f(x)}=\min_x f(x)$. In our case, $\displaystyle \min_{w} \lVert \hat{X}w - \hat{y} \rVert_2=\min_{w} \lVert \hat{X}w-\hat{y} \rVert_2 ^2$.
\vspace{3mm}

\noindent The LLS solution is obtained by finding the non-stationary points of $f(w)$, by, first, rewriting it as following
\begin{equation}
\begin{aligned}
    f(w)&=\lVert \hat{X}w-\hat{y} \rVert_2^2 =(\hat{X}w-\hat{y})^T(\hat{X}w-\hat{y}) = ((\hat{X}w)^T-\hat{y}^T)(\hat{X}w-\hat{y})\\
    &=(w^T\hat{X}^T-\hat{y}^T)(\hat{X}w-\hat{y})=w^T\hat{X}^T\hat{X}w-w^T\hat{X}^T\hat{y}-\hat{y}^T\hat{X}w+\hat{y}^T\hat{y}\\
    &\stackrel{\footnotemark}{=} w^T\hat{X}^T\hat{X}w-2\hat{y}^T\hat{X}w+\hat{y}^T\hat{y}
\end{aligned}
\end{equation}
\footnotetext{this holds because $(w^T\hat{X}^T\hat{y})^T=\hat{y}^T\hat{X}w$ and they are both scalars.}
\noindent Now, we define the gradient of $f(w)$ as
\begin{equation}
    \nabla f(w) = 2w^T\hat{X}^T\hat{X} - 2\hat{y}^T \hat{X}
\label{eq:gradient}
\end{equation}
The solution can therefore be defined by putting $\nabla f(w)=0$, thus leading us to
\begin{equation}
    w^T\hat{X}^T\hat{X} - \hat{y}^T \hat{X}=0
    \label{eq:normal_equation}
\end{equation}
which is known as the normal equation of the LLS, but the latter tends to be ill-conditioned in practice, so although using \eqref{eq:normal_equation} could be a good way to solve the task, there exist way better methods for such purpose (which is also the aim of this project).
\vspace{3mm}

\noindent Let's now define the Hessian of $f(w)$ as
\begin{equation}
    \nabla^2 f(w) = 2 \hat{X}^T \hat{X}
    \label{eq:hessian}
\end{equation}

\subsection{Convexity}\label{subsec:introduction_convexity}
By looking at the Hessian matrix \eqref{eq:hessian} we can say that is always positive definite, this statement, without taking into account the constant 2, holds for our case since $\hat{X}$ has full-column rank and also
\begin{equation}
    z^T(\hat{X}^T \hat{X}) z = (\hat{X} z)^T(\hat{X} z) = \lVert \hat{X} z \rVert_2^2>0, \hspace{3mm} \forall z \neq 0
    \label{eq:p_definite}
\end{equation}
Knowing that the Hessian is always positive definite, we can assert that LLS problems are convex. In fact, from the literature, it is known that a twice differentiable function $f: \mathbb{R}^n \xrightarrow{} \mathbb{R}$ is convex iff the Hessian $\nabla^2 f(x)$ is positive semi-definite (which, of course, is true since positive definiteness implies positive semi-definiteness), $\forall x \in \mathbb{R}^n$.
\paragraph{Global minima}
\label{par:unique_global_minima}
Thanks to \eqref{eq:p_definite} we know that the Hessian \eqref{eq:hessian} is positive definite in any point $z \in \mathbb{R}^n\backslash 0$ and, therefore, our function is strongly convex, this implies that the stationary point is a global unique minimum.

\subsection{Lipschitz continuous}\label{subsec:lipschitz}
The gradient of a function $f: \mathbb{R}^n \xrightarrow{} \mathbb{R}$ is Lipschitz continuous with Lipschitz constant $L \geq 0$ if 
\begin{equation}
    \lVert \nabla f(x) - \nabla f(y) \rVert \leq L \lVert (y - x) \rVert, \forall x,y \in \mathbb{R}^n
\end{equation}

\noindent The Lipschitz constant of the gradient of the LLS is equal to the square of the largest singular value $s_1$ of the feature matrix $\hat{X} \in \mathbb{R}^{m\times n}$ since for any $\beta_1, \beta_2 \in \mathbb{R}^{n}$:
\begin{equation}
    \lVert \nabla f(\beta_2) - \nabla f(\beta_1) \rVert \stackrel{\footnotemark}{=} \lVert \hat{X}^T \hat{X} (\beta_2 - \beta_1) \rVert \leq s_{1}^2 \lVert \beta_2 - \beta_1 \rVert
    \label{eq:gradient_constant}
\end{equation}
\footnotetext{Holds due to the gradient definition for the LLS problem as shown in \eqref{eq:gradient}, we do not consider the constant since it has no effect.}
We can show that the second part of \eqref{eq:gradient_constant} holds by exploiting the fact that the norm product is less or equal to the product of the norms: $\lVert A v \rVert \leq \lVert A \rVert * \lVert v \rVert$, therefore we can state the following
\begin{equation}
    \lVert \hat{X}^T \hat{X} (\beta_2 - \beta_1) \rVert \leq \lVert \hat{X}^T \hat{X} \rVert * \lVert \beta_2 - \beta_1 \rVert
\end{equation}
Now, for any symmetric matrix (like the one we have, $\hat{X}^T \hat{X}$), $\lVert \hat{X}^T \hat{X} \rVert$ is equal to the squared largest singular value of the matrix $\hat{X}$, corresponding to the largest eigenvalue of $\hat{X}^T \hat{X}$. In fact, lets consider the SVD factorization of $\hat{X}$ defined as $\hat{X} = U\Sigma V^T$, we can write $\hat{X}^T \hat{X} = V\Sigma U^T U \Sigma V^T = V \Sigma^2 V^T$, since U is orthogonal ($U^T U=U U^T=I$).
\vspace{3mm}

\noindent Moreover, $\hat{X}^T \hat{X}$ is symmetric and this implies that
\begin{equation}
    \lVert \hat{X}^T \hat{X} \rVert = \rho(\hat{X}^T\hat{X}) = \max_i |\lambda_i|
\end{equation}
where $\rho(\hat{X}^T\hat{X})$ is the spectral radius of $\hat{X}^T\hat{X}$. Recalling that $eig(\hat{X}^T \hat{X}) = \Sigma^2(\hat{X})$ we conclude that $\lVert \hat{X}^T \hat{X} \rVert = \max \{s_i^2 \in \Sigma^2\} = s_1^2$ since the largest singular value is the first one on the matrix diagonal, this leads us to
\begin{equation}
    \lVert \hat{X}^T \hat{X} \rVert * \lVert \beta_2 - \beta_1 \rVert = s_{1}^2 \lVert \beta_2 - \beta_1 \rVert
\end{equation}
Which is what we stated in \eqref{eq:gradient_constant}, therefore $s_{1}^2$ is the Lipschitz constant of the LLS.
